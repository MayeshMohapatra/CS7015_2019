\documentclass[solution,addpoints,12pt]{exam}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{animate}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{commath}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newenvironment{Solution}{\begin{EnvFullwidth}\begin{solution}}{\end{solution}\end{EnvFullwidth}}

\printanswers
%\unframedsolutions
\pagestyle{headandfoot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% INSTRUCTIONS %%%%%%%%%%%%%%%%%%%%%
% * Fill in your name and roll number below

% * Answer in place (after each question)

% * Use \begin{solution} and \end{solution} to typeset
%   your answers.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Fill in the details below
\def\studentName{\textbf{Your Name}}
\def\studentRoll{\textbf{Roll Number}}

\firstpageheader{CS 7015 - Deep Learning - Assignment 2}{}{\studentName, \studentRoll}
\firstpageheadrule

\newcommand{\brac}[1]{\left[ #1 \right]}
\newcommand{\curly}[1]{\left\{ #1 \right\}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\card}[1]{\left\lvert #1 \right\rvert}

\begin{document}
Instructions:
\begin{itemize}
    \itemsep0em
    \item This assignment is meant to help you grok certain concepts we will use in the course. Please don't copy solutions from any sources.
    \item Avoid verbosity.
    \item Questions marked with * are relatively difficult. Don't be discouraged if you cannot solve them right away!
    \item The assignment needs to be written in latex using the attached tex file. The solution for each question should be written in the solution block in space already provided in the tex file. \textbf{Handwritten assignments will not be accepted.}
\end{itemize}

\noindent\rule{\textwidth}{1pt}

\begin{questions}

\question Suppose, a transformation matrix A, transforms the standard basis vectors of $R^3$ as follows : \\
\\
$\begin{bmatrix}
    1  \\
    0 \\
    0
 \end{bmatrix}$
 $=>$
$\begin{bmatrix}
    3  \\
    8 \\
    0
 \end{bmatrix}
 ;
 \begin{bmatrix}
    0  \\
    1 \\
    0
 \end{bmatrix}
 =>
\begin{bmatrix}
    -4  \\
    9 \\
    7
 \end{bmatrix}
;
\begin{bmatrix}
    0  \\
    0 \\
    1
 \end{bmatrix}
 =>
\begin{bmatrix}
    -1  \\
    2 \\
    6
 \end{bmatrix}
\\$
\begin{parts}
\part If the volume of a hypothetical parallelepiped in the un-transformed space is $100 units^3$ what will be volume of this parallelepiped in the transformed space? 
\begin{Solution}

\end{Solution}

\part What will be the volume if the transformation of the basis vectors is as follows :\\ 
\\
$\begin{bmatrix}
    1  \\
    0 \\
    0
 \end{bmatrix}
 =>
\begin{bmatrix}
    1  \\
    1 \\
    3
 \end{bmatrix}
 ;
 \begin{bmatrix}
    0  \\
    1 \\
    0
 \end{bmatrix}
 =>
\begin{bmatrix}
    -1  \\
    2\\
    0
 \end{bmatrix}
;
\begin{bmatrix}
    0  \\
    0 \\
    1
 \end{bmatrix}
 =>
\begin{bmatrix}
    0  \\
    2 \\
    2
 \end{bmatrix}$
\\
\begin{Solution}

\end{Solution}

\part Comment on the uniqueness of the second transformation.
\begin{Solution}

\end{Solution}
\end{parts}

\question
If $R^3$ is represented by following basis vectors :
 $\begin{bmatrix}
    5 \\
    2 \\
    0
 \end{bmatrix}$ 
,
$\begin{bmatrix}
    8  \\
    7 \\
    -11
 \end{bmatrix}$ 
,
$\begin{bmatrix}
    -4  \\
    -9 \\
    3
 \end{bmatrix}$ 

\begin{parts}
\part Find the representation of the vector 
$\begin{pmatrix}
    -3  &
    1 &
    -2
 \end{pmatrix}^T$ 
 (as represented in standard basis) in the above basis. 
 \begin{Solution}

\end{Solution}
\part We know that, orthonormal basis simplifies this transformation to a great extent. What would be the representation of vector 
$\begin{pmatrix}
    -3  &
    1 &
    -2
 \end{pmatrix}^T$ 
 in the orthogonal basis represented by :
$\begin{bmatrix}
    5  \\
    5 \\
    1
 \end{bmatrix}$ 
,
$\begin{bmatrix}
    -5  \\
    1 \\
    5
 \end{bmatrix}$ 
,
$\begin{bmatrix}
    1  \\
    -5 \\
    5
 \end{bmatrix}$ .
 \begin{Solution}

\end{Solution}
\part Comment on the advantages of having orthonormal basis.

\begin{Solution}

\end{Solution}
\end{parts}


\question A square matrix is a Markov matrix if each entry is between zero and one and the sum along each row is one. Prove that a product of Markov matrices is Markov.
\begin{Solution}

\end{Solution}


\question  Give an example of a matrix A with the following three properties:
\begin{parts}
    \part A has eigenvalues -1 and 2.
    \part The eigenvalue -1 has eigenvector
    \begin{equation}
    \left( 
    \begin{array}{cc}
         1 \\ 2 \\ 3
    \end{array} 
    \right )
    \end{equation}
    \part The eigenvalue 2 has eigenvector 
        \begin{equation}
    \left( 
    \begin{array}{cc}
         1 \\ 1 \\ 0
    \end{array} 
    \right )
   and \left( 
    \begin{array}{cc}
         0 \\ 1 \\ 1
    \end{array} 
    \right )
    \end{equation}
\end{parts}
        \begin{Solution}
        
        \end{Solution}

\question Perform the Gram-Schmidt process on each of these basis for ${\rm I\!R^3}$. And convert the resulting orthogonal basis into orthonormal basis.

\begin{parts}
    \part 
    $
        \langle \left( \begin{array}{cc}
         2 \\ 2 \\ 2
    \end{array} \right), \left( \begin{array}{cc}
         1 \\ 0 \\ -1
    \end{array} \right),
    \left( \begin{array}{cc}
         0 \\ 3 \\ 1
    \end{array} \right)
    \rangle
    $
    
    \begin{Solution}
    
    \end{Solution}

    \part 
    $
        \langle \left( \begin{array}{cc}
         1 \\ -1 \\ 0
    \end{array} \right), \left( \begin{array}{cc}
         0 \\ 1 \\ 0
    \end{array} \right),
    \left( \begin{array}{cc}
         2 \\ 3 \\ 1
    \end{array} \right)
    \rangle
    $
    
    \begin{Solution}
    
    \end{Solution}


    
\end{parts}

\question Suppose, every year, 4\% of the birds from Canada migrate to the US, and 1\% of them travel to Mexico. Similarly, every year, 6\% of the birds from US migrate to Canada, and 4\% to Mexico. Finally, every year 10\% of the birds from Mexico migrate to the US, and 0\% go to Canada.
\begin{parts}
    \part Represent the above probabilities in a transition matrix.
        \begin{Solution}

    \end{Solution}
    \part Is it possible that after some years, the number of birds in the 3 countries will become constant?
        \begin{Solution}

    \end{Solution}
\end{parts}


\question
\begin{parts}
    \part Show that any set of four unique vectors in ${\rm I\!R^{2}}$ is linearly dependent.
    \begin{Solution}
    
    \end{Solution}
    
    \part What is the maximum number of unique vectors that a linearly independent subset of ${\rm I\!R^{2}}$ can have ?
    
\end{parts}
    \begin{Solution}
    \end{Solution}

\question 
\begin{parts}
    
\part Determine if the vectors \{$v_1, v_2, v_3$\} are linearly independent, where \\
$v_1 = $ $\begin{bmatrix}
    5  \\
    0 \\
    0
 \end{bmatrix}$, $v_2 = $ $\begin{bmatrix}
    7  \\
    2 \\
    -6
 \end{bmatrix}$, 
 $v_3 = $ $\begin{bmatrix}
    9  \\
    4 \\
    -8
 \end{bmatrix}$ \\
Justify each answer
    \begin{Solution}
    \end{Solution}
\part Prove that each set \{f, g\} is linearly independent in the vector space of all functions from ${\rm I\!R^{+}}$to ${\rm I\!R}$.
\begin{enumerate}
    \item f(x) = x and g(x) = $\frac{1}{x}$
    \item f(x) = cos(x) and g(x) = sin(x)
    \item f(x) = $e^x$ and g(x) = ln(x)
\end{enumerate}

    \begin{Solution}
    \end{Solution}
\end{parts}
\question Let $t_{\theta}$ be 
\begin{equation}
    \left( \begin{array}{cc}
     \cos{\theta} & -\sin{\theta} \\
     \sin{\theta} & \cos{\theta}
\end{array} \right)
\end{equation}\\
\begin{parts}
    \part Show that $t_{\theta_{1}+\theta_{2}} = t_{\theta_{1}}*t_{\theta_{2}} $ (* here stands for matrix multiplication).
    \begin{Solution}
    
    \end{Solution}
    \part Show that $ t_{\theta}^{-1} = t_{-\theta}. $
    
    \begin{Solution}
    
    \end{Solution}
    
\end{parts}
 

\question Given matrix has distinct eigenvalues \\ \\
 $\begin{bmatrix}
    1 & 2 & 1  \\
    6 & -1 & 0\\
    -1 & -2 & -1
 \end{bmatrix}$

 \begin{parts}
     \part Diagonalize it.
     \begin{Solution}
\end{Solution}
     \part Find a basis with respect to which this matrix has that diagonal representation
     \begin{Solution}
\end{Solution}
     \part Find the matrices P and $P^{-1}$ to effect the change of basis.
 \end{parts}
\begin{Solution}
\end{Solution}

\question * \textbf{Induced Matrix Norms}
\newline
In case you didn't already know, a norm $\|.\|$ is any function with the following properties:
\begin{enumerate}
    \item $\|x\| \geq 0$ for all vectors $x$.
    \item $\|x\| = 0 \iff x = \mathbf{0}$.
    \item $\|\alpha x\| = |\alpha| \|x\|$ for all vectors $x$, and real numbers $\alpha$.
    \item $\|x + y\| \leq \|x\| + \|y\|$ for all vectors $x, y$.
\end{enumerate}

Now, suppose we're given some vector norm $\|.\|$ (this could be L2 or L1 norm, for example). We would like to use this norm to measure the size of a matrix $A$. One way is to use the corresponding induced matrix norm, which is defined as $\|A\| = \sup_{x} \{\| Ax \| : \|x \| = 1\}$.

E.g.: $\|A\|_2 = \sup_{x} \{ \|Ax\|_2 : \|x\|_2 = 1 \}$, where $\|.\|_2$ is the standard L2 norm for vectors, defined by $\|x\|_2 = \sqrt{x^Tx}$.\\
Note: sup stands for supremum.

Prove the following properties for an arbitrary induced matrix norm:

\begin{parts}
    \part $\| A \| \geq 0$.
    \begin{Solution}
    \end{Solution}
    \part $\|\alpha A\| = |\alpha| \|A\|$ for any real number $\alpha$.
    \begin{Solution}
    \end{Solution}
    \part $\| A + B \| \leq \|A\| + \|B\|$.
    \begin{Solution}
    \end{Solution}
    \part $\| A \| = 0 \iff A = 0$.
    \begin{Solution}
    \end{Solution}
    \part $\|AB\| \leq \|A\|\|B\|$.
    \begin{Solution}
    \end{Solution}
    \part $\|A\|_2 = \sigma_{\max}(A)$, where $\sigma_{\max}$ is the largest singular value.
    \begin{Solution}
    \end{Solution}
\end{parts}

\question Prove that the eigen vectors of a real symmetric($ S_{n*n}$) matrix are linearly independent and form an orthogonal basis for $R^n$.
\begin{Solution}
\end{Solution}

\question \textbf{RAYLEIGH QUOTIENT}
\newline Let A be an n$ \times $n real symmetric matrix with eigenvalues $\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \dots \leq  \lambda_n $ and corresponding orthonormal eigenvectors $ v_1, \dots, v_n $.

\begin{parts}
    \part Show that
    \begin{equation}
        \lambda_1 = \min_{x \neq 0} \frac{\langle x, Ax \rangle}{\norm{x}^2} \; \; \; and \; \; \; \lambda_n = \max_{x \neq 0} \frac{\langle x, Ax \rangle}{\norm{x}^2}.
    \end{equation}
    Also, show directly that if $v \neq 0$ minimizes $\frac{\langle x, Ax \rangle}{\norm{x}^2}$, then v is an eigenvector of A corresponding to the minimum eigenvalue of A.
\begin{Solution}
\end{Solution}
\part show that

\begin{equation}
        \lambda_2 = \min_{x \perp v_{1}, x \neq 0} \frac{\langle x, Ax \rangle}{\norm{x}^2}.
    \end{equation}

\end{parts}

\question An m $\times$  n matrix has full row rank if its row rank is m, and it has full column rank if its column rank is n. Show that a matrix can have both full row rank and full column rank only if it is a square matrix.
\begin{Solution}
\end{Solution}


\question Let A be a $m\times n$ matrix, and suppose $\vec{v}$ and $\vec{w}$ are orthogonal eigenvectors of $A^{T}A$. Show that $A\vec{v}$ and $A\vec{w}$ are orthogonal.
\begin{Solution}
\end{Solution}

\question Let $u_{1}, u_2, ...., u_n$ be a set of $n$ orthonormal vectors. Similarly let $v_{1}, v_2, ...., v_n$ be another set of $n$ orthonormal vectors.
\begin{parts}
    \part Show that $u_{1}v_{1}^T$ is a rank-1 matrix.
    \begin{Solution}
    \end{Solution}
    
    \part Show that $u_{1}v_{1}^T + u_{2}v_{2}^T$ is a rank-2 matrix.
    \begin{Solution}
    \end{Solution}
    
    \part Show that $\sum_{i=1}^{n} u_{i}v_{i}^T$ is a rank-n matrix.
    \begin{Solution}
    \end{Solution}
\end{parts}

\end{questions}
\end{document} 